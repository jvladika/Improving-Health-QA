{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "National Library of Medicine (NLM) releases every year MEDLINE, a snapshot of all the currently available \n",
    "PubMed papers in its library (www.nlm.nih.gov/databases/download/pubmed_medline.html).\n",
    "\n",
    "The latest snapshot for 2022 includes 33.4M paper abstracts and metadata. Not all of them are suitable, so\n",
    "we removed all the papers with empty abstracts, with unfinished abstracts, and with non-English abstracts.\n",
    "\n",
    "This yields 20.6M paper instances. It can be downloaded from: https://zenodo.org/records/7849020\n",
    "\n",
    "There is a separate file for abstracts and another one for metadata (authors, journal, publication year...).\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "metadata = pd.read_csv(\"./data/pubmed_landscape_data.csv\")\n",
    "\n",
    "abstracts = pd.read_csv(\"./data/pubmed_landscape_abstracts.csv\")\n",
    "abstracts_text = abstracts.AbstractText.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code for loading the three datasets used in our experiments.\n",
    "'''\n",
    "import json\n",
    "\n",
    "#HealthFC\n",
    "healthfc_df = pd.read_csv(\"healthFC_annotated.csv\")\n",
    "hfc_questions = healthfc_df.en_claim.tolist()\n",
    "hfc_labels = healthfc_df.label.tolist()\n",
    "\n",
    "\n",
    "#TREC-Health\n",
    "trec_df = pd.read_csv(\"trec_health.csv\")\n",
    "trec_questions = trec_df.description.tolist()\n",
    "trec_labels = trec_df.label.tolist()\n",
    "\n",
    "\n",
    "#BioASQ-7b\n",
    "with open(\"BioASQ-train-yesno-7b.json\", \"r\") as f:\n",
    "    bioasq_content = json.load(f)\n",
    "    \n",
    "bioasq_questions = list()\n",
    "bioasq_answers = list()\n",
    "for qid in range(len(bioasq_content[\"data\"][0][\"paragraphs\"])):\n",
    "    question = bioasq_content[\"data\"][0][\"paragraphs\"][qid][\"qas\"][0][\"question\"]\n",
    "    answer = bioasq_content[\"data\"][0][\"paragraphs\"][qid][\"qas\"][0][\"answers\"]\n",
    "    if question not in bioasq_questions:\n",
    "        bioasq_questions.append(question)\n",
    "        bioasq_answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a sparse index for BM25 search. We use the library retriv.\n",
    "'''\n",
    "\n",
    "from retriv import SparseRetriever\n",
    "\n",
    "sr = SparseRetriever(\n",
    "  index_name=\"pubmed-index\",\n",
    "  model=\"bm25\",\n",
    "  min_df=10,\n",
    "  tokenizer=\"whitespace\",\n",
    "  stemmer=\"english\",\n",
    "  stopwords=\"english\",\n",
    "  do_lowercasing=True,\n",
    "  do_ampersand_normalization=True,\n",
    "  do_special_chars_normalization=True,\n",
    "  do_acronyms_normalization=True,\n",
    "  do_punctuation_removal=True,\n",
    ")\n",
    "\n",
    "corpus_path = \"/mnt/mydrive/PubMed/pubmed_landscape_abstracts.csv\"\n",
    "\n",
    "\n",
    "#Construct the inverted index.\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "sr = sr.index_file(\n",
    "  path=corpus_path,  # File kind is automatically inferred\n",
    "  show_progress=True,         # Default value\n",
    "  callback=lambda doc: {      # Callback defaults to None.\n",
    "    \"id\": doc[\"PMID\"],\n",
    "    \"text\": doc[\"AbstractText\"],          \n",
    "    }\n",
    "  )\n",
    "\n",
    "duration = time.time() - start\n",
    "print(duration) #Duration: 5772.615013837814\n",
    "\n",
    "\n",
    "#Serialize the inverted and index and save it as a pickled file.\n",
    "import pickle\n",
    "file = open('/mnt/mydrive/pickled_sr', 'wb')\n",
    "pickle.dump(sr, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Open the file with the pickled inverted index (sparse retriever).\n",
    "file = open('/mnt/mydrive/PubMed/pickled_sr', 'rb')\n",
    "inverted_index = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "#Load the dataset HealthFC, as an example.\n",
    "df = pd.read_csv(\"healthFC_annotated.csv\")\n",
    "claims = df.en_claim.tolist()\n",
    "labels = df.label.tolist()\n",
    "\n",
    "\n",
    "#Create a list of queries from the claims.\n",
    "query_list = list()\n",
    "idx = 0\n",
    "for claim in claims:\n",
    "    claim = claim.lower()\n",
    "    d = dict()\n",
    "    d[\"id\"] = str(idx)\n",
    "    d[\"text\"] = claim\n",
    "    query_list.append(d)\n",
    "    idx += 1\n",
    "\n",
    "#Retrieve the top 100 results for each query (claim)\n",
    "results = inverted_index.msearch(\n",
    "  queries=query_list,\n",
    "  cutoff=100,\n",
    ")\n",
    "\n",
    "#Get all the document IDs from results.\n",
    "all_ids = list()\n",
    "for claim_id, document_results in results.items():\n",
    "    ids = list()\n",
    "    for doc_id, score in document_results.items():\n",
    "        ids.append(int(doc_id))\n",
    "    all_ids.append(ids)\n",
    "    \n",
    "\n",
    "#Load all sentences of all 100 abstracts for each claim into a big list of lists.\n",
    "claim_sentences = list()\n",
    "for ids in all_ids:\n",
    "    all_sentences = list()\n",
    "    \n",
    "    for doc_id in ids:\n",
    "        abstract = abstracts_text[doc_id]\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        claim_sentences.append(sentences)\n",
    "    \n",
    "\n",
    "#Load sentence transformer for selecting evidence sentences.\n",
    "model = SentenceTransformer('copenlu/spiced')\n",
    "print(\"loaded sentence model!\")\n",
    "\n",
    "\n",
    "#Find top 3 most similar sentences to query from each document, to condense it to most important parts.\n",
    "top_sentences = list()\n",
    "for idx in range(len(claims)):\n",
    "    claim = claims[idx]\n",
    "    print(idx)\n",
    "    \n",
    "    for j in range(100):\n",
    "        start_index = idx*100 + j \n",
    "        \n",
    "        sents = claim_sentences[start_index]\n",
    "\n",
    "        sents_embeddings = model.encode(sents, convert_to_tensor=True)\n",
    "        claim_embedding = model.encode(claim, convert_to_tensor=True)\n",
    "        cos_scores = util.cos_sim(claim_embedding, sents_embeddings)[0]\n",
    "\n",
    "        new_k = 3\n",
    "        if len(sents) < 3:\n",
    "            new_k = len(sents)\n",
    "\n",
    "        top_results = torch.topk(cos_scores, k=new_k)\n",
    "\n",
    "        np_results = top_results[1].detach().cpu().numpy()\n",
    "        top_sentences.append(np_results)\n",
    "\n",
    "selected_sentences = list()\n",
    "for idx in range(len(top_sentences)):\n",
    "    top = top_sentences[idx]\n",
    "    top = np.sort(top)\n",
    "    sents = np.array(claim_sentences[idx])[top]    \n",
    "    selected_sentences.append(sents)\n",
    "  \n",
    "\n",
    "# Create a joint list of concatenated claims and evidence, in form of \"claim [SEP] evidence1 evidence2 evidence3\"\n",
    "joint_list = list()\n",
    "for idx in range(len(claims)):\n",
    "    claim = claims[idx]\n",
    "    for j in range(100):\n",
    "        start_index = idx*100 + j\n",
    "        joint = claim + \" [SEP] \"\n",
    "\n",
    "        for s in selected_sentences[start_index]:\n",
    "            joint += s\n",
    "            joint += \" \"\n",
    "        joint_list.append(joint)\n",
    "        \n",
    "\n",
    "with open(\"jointselect_healthfc_100.txt\", \"w\") as f:\n",
    "    for line in joint_list:\n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(\"jointselect_healthfc_100.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    flatlines = list()\n",
    "    for l in lines:\n",
    "        l = l.strip()\n",
    "        if \"[SEP]\" in l:\n",
    "            flatlines.append(l)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "#A PyTorch class for our dataset, containing text encodings and final labels (answers).\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "#Final QA answer prediction loop. \n",
    "def get_result(input_list, model, tokenizer):\n",
    "\n",
    "    input_encoded = tokenizer(input_list, return_tensors='pt',\n",
    "                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n",
    "\n",
    "    input_dataset = QADataset(input_encoded, np.zeros(len(input_list)))\n",
    "\n",
    "    test_loader = DataLoader(input_dataset, batch_size=16,\n",
    "                             drop_last=False, shuffle=False, num_workers=4)\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "    result = np.zeros((len(test_loader.dataset),3))\n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_num, instances in enumerate(test_loader):\n",
    "            print(batch_num)\n",
    "            input_ids = instances[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = instances[\"attention_mask\"].to(\"cuda\")\n",
    "            logits = model(input_ids=input_ids,\n",
    "                                          attention_mask=attention_mask)[0]\n",
    "            probs = logits.softmax(dim=1)\n",
    "            result[index : index + probs.shape[0]] = probs.to(\"cpu\")\n",
    "            index += probs.shape[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "#Load the NLI model.\n",
    "MODEL_NAME = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=1024)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#Run the QA final prediction loop.\n",
    "result = get_result(flatlines, model, tokenizer)\n",
    "\n",
    "\n",
    "#Load the dataset.\n",
    "df = pd.read_csv(\"healthFC_annotated.csv\")\n",
    "claims = df.en_claim.tolist()\n",
    "labels = df.label.tolist()\n",
    "\n",
    "#Store the aggregated final predictions for top 100 evidence documents, for each question.\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "with open(\"jsl_healthfc_aggregated_results.txt\", \"w\") as f:\n",
    "\n",
    "    for idx in range(len(claims)):\n",
    "        claim = claims[idx]\n",
    "        num_abs = 100\n",
    "\n",
    "        end_index += num_abs\n",
    "        predictions = result[start_index : end_index]\n",
    "        start_index += num_abs\n",
    "\n",
    "        f.write(claim + \"\\n\")\n",
    "        f.write(\"True label: \" + str(labels[idx]))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        verdicts = dict(SUP=0, NEI=0, REF=0)\n",
    "        for pred in predictions:\n",
    "            verdict = np.argmax(pred)\n",
    "            if verdict==0:\n",
    "                verdicts[\"SUP\"] += 1\n",
    "            elif verdict==1:\n",
    "                verdicts[\"NEI\"] += 1\n",
    "            elif verdict==2:\n",
    "                verdicts[\"REF\"] += 1\n",
    "\n",
    "        f.write(\"Verdicts: \" + str(verdicts) + \"\\n\")\n",
    "        f.write(\"Mean: \" + str(np.mean(predictions, axis=0)) + \"\\n\\n\")\n",
    "\n",
    "#Store the probabilities of the three final predictions for top 100 evidence documents, for each question.\n",
    "with open(\"jsl_healthfc_label_probabilities.txt\", \"w\") as f:\n",
    "    for idx in range(len(result)):\n",
    "        fl = flatlines[idx]\n",
    "        r = result[idx]\n",
    "        f.write(fl + \"\\n\" + str(list(r)))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with TOP K, publication year, number of citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load all the publication years of all top-100 documents for each question.\n",
    "'''\n",
    "\n",
    "import ast \n",
    "\n",
    "#Collect  all IDs of top 100 documents for each question in this dataset.\n",
    "with open(\"results_healthfc_100.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "flag = 0\n",
    "dataset_ids = list()\n",
    "for l in lines:\n",
    "    l = l.strip()\n",
    "    if flag%4==2:\n",
    "        ids = ast.literal_eval(l)\n",
    "        dataset_ids.extend(ids)\n",
    "    flag += 1\n",
    "\n",
    "all_ids = list()\n",
    "for ids in dataset_ids:\n",
    "    all_ids.extend(ids)\n",
    "\n",
    "\n",
    "#Load the file with all documents' metadata.\n",
    "metadata = pd.read_csv(\"/mnt/mydrive/PubMed/pubmed_landscape_data.csv\")\n",
    "\n",
    "#Create a list of all years.\n",
    "YEARS = list()\n",
    "for idx in all_ids:\n",
    "    y = metadata[metadata.PMID == idx].Year.values\n",
    "    if len(y) == 0:\n",
    "        y = 0\n",
    "    else:\n",
    "        y = int(y[0])\n",
    "    YEARS.append(y)\n",
    "\n",
    "YEARS[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the number of citations for each of the top-10 documents for each question.\n",
    "\n",
    "Since this parameter is not available in the original metadata, we use the Semantic Scholar API to query the number of citations in their knowledge base.\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "#Collect the PubMed IDs (PMIDs) from all top 100 documents for each question in this dataset.\n",
    "all_claims = list()\n",
    "all_ids = list()\n",
    "all_scores = list()\n",
    "with open(\"results_healthfc_100.txt\", \"r\") as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "\n",
    "    idx = 0\n",
    "    for line in lines:\n",
    "        if idx%4==0:\n",
    "            claim = line\n",
    "            all_claims.append(claim)\n",
    "        elif idx%4==1:\n",
    "            all_scores = ast.literal_eval(line)\n",
    "        elif idx%4==2:\n",
    "            ids = ast.literal_eval(line)\n",
    "            all_ids.append(ids)\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "metadata = pd.read_csv(\"/mnt/mydrive/PubMed/pubmed_landscape_data.csv\")\n",
    "pmids = metadata.PMID.tolist()\n",
    "\n",
    "NUM_CITATIONS = list()\n",
    "total = 0\n",
    "for ids in all_ids:\n",
    "\n",
    "    for doc_id in ids[:10]:\n",
    "        pmid = pmids[doc_id]\n",
    "\n",
    "        # Get the number of citations of the paper from Semantic Scholar based on its PMID.\n",
    "        url = \"https://api.semanticscholar.org/graph/v1/paper/PMID:\" + str(pmid)\n",
    "        resp = requests.get(url=url)\n",
    "        data = resp.json()\n",
    "        if \"error\" in data:\n",
    "            NUM_CITATIONS.append(0)\n",
    "            continue\n",
    "            \n",
    "        ss_pid = data[\"paperId\"]\n",
    "        total += 1\n",
    "        \n",
    "        # Each page only displays up to 100 citations.\n",
    "        # If there are multiple pages, then keep going until reaching the last. Add 100 citations per page.\n",
    "        total_citations = 0\n",
    "        idx = 0\n",
    "        count = 100\n",
    "        while count == 100:\n",
    "            offset = idx*100\n",
    "            url = \"https://api.semanticscholar.org/graph/v1/paper/\" + \\\n",
    "                    str(ss_pid) + \"/citations?fields=title,authors&offset=\" + str(offset)\n",
    "            \n",
    "            resp = requests.get(url=url)\n",
    "            data = resp.json()\n",
    "            if \"error\" in data:\n",
    "                NUM_CITATIONS.append(total_citations)\n",
    "                break\n",
    "            \n",
    "            count = len(data[\"data\"])\n",
    "            total_citations += count\n",
    "            total += 1\n",
    "            idx += 1\n",
    "\n",
    "        NUM_CITATIONS.append(total_citations)\n",
    "    \n",
    "    if len(NUM_CITATIONS)%1000==0:\n",
    "        print(NUM_CITATIONS)\n",
    "    \n",
    "NUM_CITATIONS[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experiments with top k, with publication year, and with citation count.\n",
    "'''\n",
    "import ast\n",
    "\n",
    "\n",
    "#First load the prediction probabilities.\n",
    "with open(\"jsl_bioasq_ss_finetunede6_linesprobs.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    #lines = [l for l in lines if \"[SEP]\" in l]\n",
    "\n",
    "prediction_probabilities = list()\n",
    "flag = 0\n",
    "for l in lines:\n",
    "    l = l.strip()\n",
    "    if flag%2==1:\n",
    "        l = l.replace(\"    \", \" \").replace(\"   \", \" \").replace(\"  \", \" \")\n",
    "        l = l.replace(\" \", \", \").replace(\",,\", \", \")\n",
    "        prediction_probabilities.append(ast.literal_eval(l))\n",
    "    flag += 1\n",
    "    \n",
    "\n",
    "prediction_counts = list()\n",
    "\n",
    "TOP_K = 10             #Set the maximum number of evidence documents to retrieve.\n",
    "MIN_YEAR = 2015        #Set the lowest cut-off publication year for retrieved articles. \n",
    "MIN_CITATIONS = 50     #Set the lowest cut-off number of citations for retrieved articles.\n",
    "\n",
    "\n",
    "prediction_counts = list()\n",
    "for idx in range(len(prediction_probabilities)):\n",
    "    all_probs = prediction_probabilities[idx]\n",
    "\n",
    "    if idx %100==0:\n",
    "        if idx != 0:\n",
    "            prediction_counts.append(all_predictions)\n",
    "        all_predictions = np.array([0, 0, 0])\n",
    "\n",
    "    # Retrieve only the top k documents. If larger index than top_k, skip the rest of the documents.\n",
    "    if idx%100 >= TOP_K:\n",
    "        continue\n",
    "\n",
    "    #Retrieve only the documents released on the or after the minimum year.\n",
    "    if YEARS[idx] < MIN_YEAR:\n",
    "        continue\n",
    "\n",
    "    # Retrieve only the documents with at least the minimum number of citations.\n",
    "    ceil_idx = idx//100\n",
    "    inside_idx = idx%10\n",
    "    paper_citations = NUM_CITATIONS[ceil_idx + inside_idx] \n",
    "    if paper_citations < MIN_CITATIONS:\n",
    "        continue\n",
    "\n",
    "    verdict = np.array(all_probs).argmax()\n",
    "    all_predictions[verdict] += 1\n",
    "\n",
    "prediction_counts.append(all_predictions)\n",
    "\n",
    "majority_votes = list()\n",
    "BINARY = True\n",
    "TERNARY = False\n",
    "\n",
    "for counts in prediction_counts:\n",
    "\n",
    "    #If there are only two labels, just look at entailment and contradiction predictions.\n",
    "    if BINARY:\n",
    "        if counts[0] > counts[2]:\n",
    "            majority_votes.append(1)\n",
    "        if counts[2] > counts[0]:\n",
    "            majority_votes.append(0)\n",
    "        if counts[0] == counts[2]:\n",
    "            majority_votes.append(0)\n",
    "\n",
    "    #If there are three labels, take the highest scoring class amonf the three predictions.\n",
    "    elif TERNARY:\n",
    "        v = np.array(counts).argmax()  \n",
    "        if np.sum(counts) == 0:\n",
    "            majority_votes.append(1)\n",
    "            continue\n",
    "\n",
    "        majority_votes.append(v)\n",
    "\n",
    "majority_votes = np.array(majority_votes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Print the results of the experiments.\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def print_scores(actual_values, predicted_values):\n",
    "    # Example arrays\n",
    "    #actual_values = yesno_labels\n",
    "    #predicted_values = result\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(actual_values, predicted_values, average = \"macro\")\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(actual_values, predicted_values, average = \"macro\")\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(actual_values, predicted_values, average = \"macro\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(actual_values, predicted_values)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "predicted = majority_votes\n",
    "true_labels = labels\n",
    "print_scores(true_labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with TOP J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The remaining cells focus on the experiments for running the TOP J tests.\n",
    "'''\n",
    "\n",
    "import ast\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load all claims and document IDs from the results file.\n",
    "all_claims = list()\n",
    "all_ids = list()\n",
    "with open(\"openhealth_results.txt\", \"r\") as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    idx = 0\n",
    "    for line in lines:\n",
    "        if idx%4==0:\n",
    "            claim = line\n",
    "            all_claims.append(claim)\n",
    "        elif idx%4==1:\n",
    "            scores = ast.literal_eval(line)\n",
    "        elif idx%4==2:\n",
    "            ids = ast.literal_eval(line)\n",
    "            all_ids.append(ids)\n",
    "        \n",
    "        idx += 1\n",
    "\n",
    "\n",
    "#Load all PubMed abstracts.\n",
    "abstracts = pd.read_csv(\"/mnt/mydrive/PubMed/pubmed_landscape_abstracts.csv\")\n",
    "abstracts_text = abstracts.AbstractText.tolist()\n",
    "\n",
    "\n",
    "#Load all sentences of top 20 abstracts for each claim into a big list of lists.\n",
    "claim_sentences = list()\n",
    "for ids in all_ids:\n",
    "    all_sentences = list()\n",
    "    \n",
    "    for doc_id in ids[:20]:\n",
    "        abstract = abstracts_text[doc_id]\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        all_sentences.extend(sentences)\n",
    "        all_sentences = [s.lower() for s in all_sentences]   \n",
    "    claim_sentences.append(all_sentences)\n",
    "    \n",
    "\n",
    "#Load the sentence transformer for selecting evidence sentences.\n",
    "model = SentenceTransformer('copenlu/spiced')\n",
    "\n",
    "\n",
    "#Find top 20 sentences for each claim. \n",
    "#Later, a subset of top-j sentences (1, 3, 5, 10...) will be chosen out of these 20 in experiments.\n",
    "top_sentences = list()\n",
    "for idx in range(len(all_claims)):\n",
    "    claim = all_claims[idx]\n",
    "    print(idx)\n",
    "    sents = claim_sentences[idx]\n",
    "    \n",
    "    sents_embeddings = model.encode(sents, convert_to_tensor=True)\n",
    "    claim_embedding = model.encode(claim, convert_to_tensor=True)\n",
    "    cos_scores = util.cos_sim(claim_embedding, sents_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=20)\n",
    "    \n",
    "    np_results = top_results[1].detach().cpu().numpy()\n",
    "    top_sentences.append(np_results)\n",
    "\n",
    "\n",
    "selected_sentences = list()\n",
    "for idx in range(len(all_claims)):\n",
    "    top = top_sentences[idx]\n",
    "   \n",
    "    top = np.sort(top)\n",
    "    sents = np.array(claim_sentences[idx])[top]    \n",
    "\n",
    "    selected_sentences.append(sents)\n",
    "  \n",
    " # Create a joint list of concatenated claims and evidence, in form of \"claim [SEP] evidence1 ||| evidence2 ... ||| evidence20\"\n",
    "joint_list = list()\n",
    "for idx in range(len(all_claims)):\n",
    "    joint = all_claims[idx] + \" [SEP] \"\n",
    "    for s in selected_sentences[idx]:\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "        joint += s\n",
    "        joint += \" ||| \"\n",
    "    joint_list.append(joint)\n",
    "\n",
    "    \n",
    "#Save this in a file before the final step.\n",
    "with open(\"healthfc_joint20.txt\", \"w\") as f:\n",
    "\tfor example in joint_list:\n",
    "\t\tf.write(example)\n",
    "\t\tf.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TOP J experiments\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the stored files from the previous step\n",
    "with open(\"healthfc_joint20.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    flatlines = list()\n",
    "    for l in lines:\n",
    "        l = l.strip()\n",
    "        if \"[SEP]\" in l:\n",
    "            flatlines.append(l)\n",
    "\n",
    "#Load the lines and split the 20 sentences up.         \n",
    "jointlines = list()\n",
    "for fl in flatlines:\n",
    "    claim, evs = fl.split(\" [SEP] \")\n",
    "    evidences = evs.split(\" ||| \")\n",
    "\n",
    "    for ev in evidences:\n",
    "        new_string = claim + \" [SEP] \"\n",
    "        new_string += ev\n",
    "        jointlines.append(new_string)\n",
    "\n",
    "#Torch dataset used in the prediction pipeline.\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "#Final QA answer prediction loop. \n",
    "def get_result(input_list, model, tokenizer):\n",
    "\n",
    "    input_encoded = tokenizer(input_list, return_tensors='pt',\n",
    "                             truncation_strategy='only_first', add_special_tokens=True, padding=True)\n",
    "\n",
    "    input_dataset = QADataset(input_encoded, np.zeros(len(input_list)))\n",
    "\n",
    "    test_loader = DataLoader(input_dataset, batch_size=16,\n",
    "                             drop_last=False, shuffle=False, num_workers=4)\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "    result = np.zeros((len(test_loader.dataset),3))\n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_num, instances in enumerate(test_loader):\n",
    "            print(batch_num)\n",
    "            input_ids = instances[\"input_ids\"].to(\"cuda\")\n",
    "            attention_mask = instances[\"attention_mask\"].to(\"cuda\")\n",
    "            logits = model(input_ids=input_ids,\n",
    "                                          attention_mask=attention_mask)[0]\n",
    "            probs = logits.softmax(dim=1)\n",
    "            result[index : index + probs.shape[0]] = probs.to(\"cpu\")\n",
    "            index += probs.shape[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "#Load the NLI model.\n",
    "MODEL_NAME = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=1024)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#Run the QA final prediction loop.\n",
    "result = get_result(flatlines, model, tokenizer)\n",
    "\n",
    "#Load the dataset with questions (claims) and labels.\n",
    "df = pd.read_csv(\"healthFC_annotated.csv\")\n",
    "claims = df.en_claim.tolist()\n",
    "labels = df.label.tolist()\n",
    "\n",
    "\n",
    "yesno_indices = np.where(np.array(labels) != 1)[0]\n",
    "yesno_claims = claims\n",
    "yesno_labels = labels\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "\n",
    "#Generate a file with results (overall verdicts for each pair of \"question + evidence_sentence\")\n",
    "with open(\"topj_20sentences_healthfc_results.txt\", \"w\") as f:\n",
    "\n",
    "    for idx in range(len(yesno_claims)):\n",
    "        claim = yesno_claims[idx]\n",
    "        num_abs = 20\n",
    "\n",
    "        end_index += num_abs\n",
    "        predictions = result[start_index : end_index]\n",
    "        start_index += num_abs\n",
    "\n",
    "        f.write(claim + \"\\n\")\n",
    "        f.write(\"True label: \" + str(yesno_labels[idx]))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        verdicts = dict(SUP=0, NEI=0, REF=0)\n",
    "        for pred in predictions:\n",
    "            verdict = np.argmax(pred)\n",
    "            if verdict==0:\n",
    "                verdicts[\"SUP\"] += 1\n",
    "            elif verdict==1:\n",
    "                verdicts[\"NEI\"] += 1\n",
    "            elif verdict==2:\n",
    "                verdicts[\"REF\"] += 1\n",
    "\n",
    "        f.write(\"Verdicts: \" + str(verdicts) + \"\\n\")\n",
    "        f.write(\"Mean: \" + str(np.mean(predictions, axis=0)) + \"\\n\\n\")\n",
    "        \n",
    "\n",
    "#Generate the label prediction probabilitiees for each of the top 20 sentences selected (so for \"question + evidence_sentence\" pairs).\n",
    "#In the experiments, top j is set to 1, 3, 5, 10... and then only top-j sentences are selected for the majority vote.\n",
    "with open(\"topj_20sentences_healthfc_probabilities.txt\", \"w\") as f:\n",
    "    for idx in range(len(result)):\n",
    "        fl = jointlines[idx]\n",
    "        r = result[idx]\n",
    "        f.write(fl + \"\\n\" + str(list(r)))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experiments similar to TOP K, just change some parameters in the main loop.\n",
    "'''\n",
    "\n",
    "TOP_J = 5 \n",
    "\n",
    "prediction_counts = list()\n",
    "for idx in range(len(prediction_probabilities)):\n",
    "    all_probs = prediction_probabilities[idx]\n",
    "\n",
    "    #Only 20 sentences per questions, so change %100 to %20.\n",
    "    if idx%20==0:\n",
    "        if idx != 0:\n",
    "            prediction_counts.append(all_predictions)\n",
    "        all_predictions = np.array([0, 0, 0])\n",
    "\n",
    "    # Do the count with only the TOP-J sentences.\n",
    "    if idx%20 >= TOP_J:\n",
    "        continue\n",
    "\n",
    "    verdict = np.array(all_probs).argmax()\n",
    "    all_predictions[verdict] += 1\n",
    "\n",
    "prediction_counts.append(all_predictions)\n",
    "\n",
    "majority_votes = list()\n",
    "BINARY = True\n",
    "TERNARY = False\n",
    "\n",
    "for counts in prediction_counts:\n",
    "\n",
    "    #If there are only two labels, just look at entailment and contradiction predictions.\n",
    "    if BINARY:\n",
    "        if counts[0] > counts[2]:\n",
    "            majority_votes.append(1)\n",
    "        if counts[2] > counts[0]:\n",
    "            majority_votes.append(0)\n",
    "        if counts[0] == counts[2]:\n",
    "            majority_votes.append(0)\n",
    "\n",
    "    #If there are three labels, take the highest scoring class amonf the three predictions.\n",
    "    elif TERNARY:\n",
    "        v = np.array(counts).argmax()  \n",
    "        if np.sum(counts) == 0:\n",
    "            majority_votes.append(1)\n",
    "            continue\n",
    "\n",
    "        majority_votes.append(v)\n",
    "\n",
    "majority_votes = np.array(majority_votes)\n",
    "\n",
    "predicted = majority_votes\n",
    "true_labels = labels\n",
    "print_scores(true_labels, predicted)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
